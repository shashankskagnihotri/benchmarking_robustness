{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.0)\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "directory_weather_path = 'validate_weather/'\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "weather_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/[jlx]_metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    if model_name == \"maskflownet\" and os.path.basename(os.path.dirname(file_path)).split(\"_\")[1] == \"s\":\n",
    "        model_name = \"maskflownet_s\"\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[2].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        all_experiments.append(experiment_flat)\n",
    "\n",
    "\n",
    "\n",
    "for file_path in glob.glob(os.path.join(directory_weather_path, '**/metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    if model_name == \"maskflownet\" and os.path.basename(os.path.dirname(file_path)).split(\"_\")[1] == \"s\":\n",
    "        model_name = \"maskflownet_s\"\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[2].split(\".\")[0]\n",
    "    weather_type = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        experiment_flat['weather_type'] = weather_type\n",
    "        # Append the processed experiment to the list\n",
    "        #all_experiments.append(experiment_flat)\n",
    "        weather_experiments.append(experiment_flat)\n",
    "\n",
    "\n",
    "df_weather = pd.DataFrame(weather_experiments)\n",
    "df_weather.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "df_weather.drop(\"data\", axis=\"columns\", inplace=True)\n",
    "df_weather['end_time'] = pd.to_datetime(df_weather['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "df_weather = df_weather.sort_values(by='end_time')\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack','loss', 'optimizer', 'learn', 'alph', 'rendering', 'transparency', 'depth', 'scene', 'recolor', 'do', 'motionblur', 'flakesize', 'constant' , 'motion', 'flake', 'frame', 'no', 'lr', 'unregistered', 'num', 'targeted', 'target', 'dataset']\n",
    "df_weather = df_weather.drop_duplicates(subset=unique_columns, keep='last')\n",
    "#df_weather.to_csv(\"test.csv\")\n",
    "\n",
    "# Replace optim=NaN with ground truth\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "df_weather['model'] = df_weather['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "df_weather['model'] = df_weather['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "df_weather['model'] = df_weather['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "df_weather.loc[(df_weather[\"dataset\"] == \"kitti-2015\") & (df_weather[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "df = pd.DataFrame(all_experiments)\n",
    "df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set iterations and epsilon to 0 where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'iterations'] = 0\n",
    "df.loc[df['attack'] == 'none', 'epsilon'] = 0\n",
    "# Replace 'epe_orig_preds' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_orig_preds'] = df['epe']\n",
    "# Replace 'epe_ground_truth' with 'epe' where 'attack' is 'none'\n",
    "df.loc[df['attack'] == 'none', 'epe_ground_truth'] = df['epe']\n",
    "\n",
    "# Get unique attack types excluding 'none'\n",
    "attack_types = df['attack'].unique()\n",
    "attack_types = attack_types[attack_types != 'none']\n",
    "\n",
    "# Filter entries with attack == 'none'\n",
    "none_entries = df[df['attack'] == 'none']\n",
    "\n",
    "# Create copies of 'none' entries for each attack type\n",
    "new_entries = []\n",
    "for attack in attack_types:\n",
    "    temp = none_entries.copy()\n",
    "    temp['attack'] = attack\n",
    "    new_entries.append(temp)\n",
    "\n",
    "# Combine all new entries into a single DataFrame\n",
    "new_entries_df = pd.concat(new_entries)\n",
    "\n",
    "# Combine the new entries with the original DataFrame\n",
    "result_df = pd.concat([df, new_entries_df])\n",
    "\n",
    "df = df.rename(columns={'corruption': '3dcc_corruption', 'intensity': '3dcc_intensity'})\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "df = df.sort_values(by='end_time')\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim', 'name', 'severity', '3dcc_corruption', '3dcc_intensity']\n",
    "df = df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "# Replace optim=NaN with ground truth\n",
    "df['optim'] = df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "df['model'] = df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "df['model'] = df['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "df['model'] = df['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "df.loc[(df[\"dataset\"] == \"kitti-2015\") & (df[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "\n",
    "# Find the missing columns in each DataFrame\n",
    "df_missing_cols = [col for col in df_weather.columns if col not in df.columns]\n",
    "df_weather_missing_cols = [col for col in df.columns if col not in df_weather.columns]\n",
    "\n",
    "# Add missing columns to df and fill them with NaN\n",
    "for col in df_missing_cols:\n",
    "    df[col] = np.nan\n",
    "\n",
    "# Add missing columns to df_weather and fill them with NaN\n",
    "for col in df_weather_missing_cols:\n",
    "    df_weather[col] = np.nan\n",
    "\n",
    "# Reorder columns in both dataframes to match (optional, but helpful for clean appending)\n",
    "df_weather = df_weather[df.columns]  # reorder df columns to match df_weather\n",
    "\n",
    "# Append the two DataFrames\n",
    "df = pd.concat([df, df_weather], ignore_index=True)\n",
    "\n",
    "#df.to_csv(\"big_df.csv\")\n",
    "big_df = df\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory_path = 'validate/'\n",
    "\n",
    "# List to store the processed data\n",
    "all_experiments = []\n",
    "\n",
    "# Iterate through all JSON files in the directory\n",
    "for file_path in glob.glob(os.path.join(directory_path, '*/[jlx]_iteration_metrics_*.json'), recursive=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract the model name from the file path\n",
    "    model_name = os.path.basename(os.path.dirname(file_path)).split(\"_\")[0]\n",
    "    if model_name == \"maskflownet\" and os.path.basename(os.path.dirname(file_path)).split(\"_\")[1] == \"s\":\n",
    "        model_name = \"maskflownet_s\"\n",
    "    dataset_name = os.path.basename(file_path).split(\"_\")[3].split(\".\")[0]\n",
    "    \n",
    "    for experiment in data['experiments']:\n",
    "        # Flatten the dictionary to include metrics at the top level\n",
    "        experiment_flat = {**experiment, **experiment.pop('metrics')}\n",
    "        # Add the model name and dataset name to the experiment data\n",
    "        experiment_flat['model'] = model_name\n",
    "        experiment_flat['dataset'] = dataset_name\n",
    "        # Append the processed experiment to the list\n",
    "        experiment_flat_key_rename = {}\n",
    "        for key, value in experiment_flat.items():\n",
    "            if key.startswith(\"epe_ground_truth_\"):\n",
    "                new_key = key.replace(\"epe_ground_truth_\", \"epe_gt_\")\n",
    "                experiment_flat_key_rename[new_key] = value\n",
    "            else:\n",
    "                experiment_flat_key_rename[key] = value\n",
    "        #print(experiment_flat_key_rename)\n",
    "        all_experiments.append(experiment_flat_key_rename)\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "iterations_df = pd.DataFrame(all_experiments)\n",
    "iterations_df.drop(\"metrics\", axis=\"columns\", inplace=True)\n",
    "\n",
    "# Keep only most recent results\n",
    "# Convert 'start_time' to datetime\n",
    "iterations_df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "# Sort the DataFrame by 'start_time'\n",
    "iterations_df = iterations_df.sort_values(by='end_time')\n",
    "\n",
    "# Drop duplicates, keeping the most recent entry for each combination of specified columns\n",
    "unique_columns = ['model', 'checkpoint', 'attack', 'targeted', 'target', 'dataset', 'norm', 'epsilon', 'iterations', 'alpha', 'optim']\n",
    "iterations_df = iterations_df.drop_duplicates(subset=unique_columns, keep='last')\n",
    "\n",
    "\n",
    "\n",
    "# Add i0 to the iterations dataframe\n",
    "none_df = df[df['attack'] == 'none']\n",
    "\n",
    "# Select only the necessary columns for the join\n",
    "none_df = none_df[['model', 'checkpoint', 'dataset', 'epe', \"epe_initial_to_negative\", \"epe_initial_to_zero\"]]\n",
    "none_df.rename(columns={'epe': 'epe_gt_i0'}, inplace=True)\n",
    "\n",
    "iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Perform the join operation to add `epe_i0` to `iterations_df`\n",
    "iterations_df = pd.merge(iterations_df, none_df, on=['model', 'checkpoint', 'dataset'], how='left')\n",
    "\n",
    "iterations_df['epe_target_i0'] = iterations_df.apply(\n",
    "    lambda row: row['epe_initial_to_negative'] if row['target'] == 'negative' else row['epe_initial_to_zero'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Sort the dataframe by 'start_time' to ensure it remains ordered\n",
    "iterations_df = iterations_df.sort_values(by='start_time')\n",
    "\n",
    "iterations_df = iterations_df.dropna(subset=['epe_gt_i20'])\n",
    "# Replace optim=NaN with ground truth\n",
    "iterations_df['optim'] = iterations_df['optim'].fillna(\"ground_truth\")\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "iterations_df['model'] = iterations_df['model'].replace('ms', 'ms_raft+')\n",
    "\n",
    "# Replace ms with ms_raft+\n",
    "iterations_df['model'] = iterations_df['model'].replace('videoflow', 'videoflow_bof')\n",
    "\n",
    "iterations_df['model'] = iterations_df['model'].replace('irr', 'irr_pwc')\n",
    "\n",
    "iterations_df.loc[(iterations_df[\"dataset\"] == \"kitti-2015\") & (iterations_df[\"model\"] == \"liteflownet3\"), \"model\"] = \"liteflownet3_pseudoreg\"\n",
    "\n",
    "# Display the updated dataframe\n",
    "#iterations_df.to_csv(\"iteration_df.csv\")\n",
    "\n",
    "# Add missing columns to df1 with NaN values\n",
    "#for col in missing_columns:\n",
    "#    big_df[col] = np.nan\n",
    "# Key columns excluding 'start_time' and 'end_time'\n",
    "key_columns = ['model', 'checkpoint', 'attack', 'norm', 'epsilon',\n",
    "               'targeted', 'target', 'loss', 'dataset',\n",
    "               'iterations', 'alpha',\n",
    "               'optim', 'boxconstraint']\n",
    "\n",
    "# Perform an outer merge on all key columns\n",
    "merged_df = pd.merge(iterations_df, big_df, how='outer', on=key_columns, suffixes=('_iter', '_big'))\n",
    "\n",
    "# Retain only 'start_time' and 'end_time' from 'big_df'\n",
    "# First, rename the 'start_time_big' and 'end_time_big' to 'start_time' and 'end_time'\n",
    "merged_df['start_time'] = merged_df['start_time_big']\n",
    "merged_df['end_time'] = merged_df['end_time_big']\n",
    "merged_df['duration'] = merged_df['duration_big']\n",
    "#merged_df['epe_initial_to_negative'] = merged_df['epe_initital_to_zero']\n",
    "\n",
    "# Drop the other 'start_time' and 'end_time' columns from 'iterations_df' (i.e., '_iter' suffixed columns)\n",
    "merged_df.drop(columns=['start_time_iter', 'end_time_iter', 'start_time_big', 'end_time_big', 'duration_big', 'duration_iter'], inplace=True)\n",
    "\n",
    "# The resulting DataFrame will now have only the 'start_time' and 'end_time' from big_df\n",
    "columns_to_process = ['epe_initial_to_zero', 'epe_initial_to_negative']\n",
    "\n",
    "# Loop through each column and apply the logic\n",
    "for col in columns_to_process:\n",
    "    # Create the new column without suffix\n",
    "    merged_df[col] = merged_df[col + '_big'].combine_first(merged_df[col + '_iter'])\n",
    "    \n",
    "    # Drop the _big and _iter columns now that we've merged them\n",
    "    merged_df.drop(columns=[col + '_big', col + '_iter'], inplace=True)\n",
    "\n",
    "# Now the dataframe contains only the new merged columns without suffixes\n",
    "print(merged_df)\n",
    "def map_dataset_values(value):\n",
    "    if value == 'final':\n",
    "        return 'sintel-final'\n",
    "    elif value == 'clean':\n",
    "        return 'sintel-clean'\n",
    "    elif value == '2015':\n",
    "        return 'kitti-2015'\n",
    "    else:\n",
    "        return value  # Leave the value unchanged if it doesn't match\n",
    "\n",
    "# Apply the function to the 'dataset' column\n",
    "merged_df['dataset'] = merged_df['dataset'].apply(map_dataset_values)\n",
    "\n",
    "merged_df = merged_df.sort_values(by=['model', 'dataset', 'norm', 'attack'], ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "merged_df.to_csv(\"one_single.csv\")\n",
    "\n",
    "weather_df = merged_df[merged_df['attack'] == 'weather']\n",
    "\n",
    "# Save the filtered data to a CSV file\n",
    "weather_df.to_csv('weather_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "combinations_list = [\n",
    "    #Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    #Top 9 models configuration\n",
    "    {'targeted': False, 'attack': 'cospgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "    {'targeted': False, 'attack': 'pgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    {'attack': 'common_corruptions', 'name': 'gaussian_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'shot_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'impulse_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'defocus_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'glass_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'motion_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'zoom_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'snow', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'frost', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'fog', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'brightness', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'contrast', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'elastic_transform', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'pixelate', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'jpeg_compression', 'severity': 3.0},\n",
    "\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'far_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'near_focus', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'fog_3d', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'color_quant', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'iso_noise', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'low_light', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'xy_motion_blur', '3dcc_intensity': 3.0},\n",
    "    {'attack': '3dcc', '3dcc_corruption': 'z_motion_blur', '3dcc_intensity': 3.0},\n",
    "\n",
    "\n",
    "    {'attack': 'none'},\n",
    "\n",
    "]\n",
    "\n",
    "# Set the dataset and norm\n",
    "#datasets = [\"sintel-final\", \"sintel-clean\", \"kitti-2015\"]\n",
    "# datasets = [\"kitti-2015\"]\n",
    "datasets = [\"sintel-final\", \"sintel-clean\"]\n",
    "\n",
    "# Define the datasets and corresponding model names\n",
    "kitti_model_names = [\n",
    "    \"raft\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"csflow\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"flow1d\", \"flowformer\", \"flowformer++\", \n",
    "    \"gmflow\", \"gmflownet\", \"hd3\", \"irr_pwc\", \"liteflownet\", \n",
    "    \"liteflownet3_pseudoreg\", \"llaflow\", \"ms_raft+\", \n",
    "    \"rapidflow\", \"scopeflow\", \"skflow\", \n",
    "    \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "sintel_model_names = [\n",
    "    \"raft\", \"pwcnet\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"maskflownet_s\", \"flow1d\", \"flowformer\", \n",
    "    \"flowformer++\", \"gmflow\", \"hd3\", \"liteflownet\", \n",
    "    \"liteflownet2\", \"liteflownet3\", \"llaflow\", \n",
    "    \"ms_raft+\", \"rapidflow\", \"scopeflow\", \n",
    "    \"skflow\", \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "kitti_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3_pseudoreg\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "sintel_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the casting dictionary\n",
    "attributes_to_cast = {\n",
    "    'attack': str,\n",
    "    'targeted': 'boolean',  # Can use a boolean cast, True/False or None\n",
    "    'iterations': float,\n",
    "    'alpha': float,\n",
    "    'loss': str,\n",
    "    'epsilon': float,\n",
    "    'target': str,\n",
    "    'norm': str,\n",
    "    'name': str,\n",
    "    'severity': float\n",
    "}\n",
    "\n",
    "# Function to cast columns in merged_df\n",
    "def cast_columns(merged_df, attributes_to_cast):\n",
    "    for col, dtype in attributes_to_cast.items():\n",
    "        if dtype == 'boolean':\n",
    "            merged_df[col] = merged_df[col].astype('bool', errors='ignore')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].astype(dtype, errors='ignore')\n",
    "    return merged_df\n",
    "\n",
    "# Cast the columns\n",
    "merged_df = cast_columns(merged_df, attributes_to_cast)\n",
    "\n",
    "# Function to match combinations\n",
    "def combination_matches_row(input_df, combination_list, output_df, model, dataset):\n",
    "    # Iterate over each combination in the combination list\n",
    "    for combination in combination_list:\n",
    "        combination_in_input_df = False\n",
    "\n",
    "        # **Check if \"optim\" exists in combination and restrict it to top 9 models**\n",
    "        if \"optim\" in combination.keys() and combination[\"optim\"] == \"initial_flow\":\n",
    "            if dataset == \"kitti-2015\":\n",
    "                if model not in kitti_model_names_top_9:\n",
    "                    continue  # Skip this combination if the model is not in top 9 for kitti-2015\n",
    "            else:\n",
    "                if model not in sintel_model_names_top_9:\n",
    "                    continue  # Skip this combination if the model is not in top 9 for other datasets\n",
    "\n",
    "        # **Check for common corruptions only for kitti-2015 dataset**\n",
    "        #if combination['attack'] == 'common_corruptions' and dataset != 'kitti-2015':\n",
    "            #continue  # Skip common corruptions if not kitti-2015 dataset\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in input_df.iterrows():\n",
    "            row_in_combination = True  # Assume the row matches the combination initially\n",
    "            row_dict = row.to_dict()\n",
    "\n",
    "            # Loop through each key-value pair in the combination\n",
    "            for key, value in combination.items():\n",
    "                if key not in row_dict.keys() or row_dict[key] != value:\n",
    "                    row_in_combination = False\n",
    "                    break  # Exit the inner loop because we know this row doesn't match\n",
    "\n",
    "            # If row_in_combination is still True after the inner loop, it means the row matches the combination\n",
    "            if row_in_combination:\n",
    "                combination_in_input_df = True\n",
    "                break  # No need to continue checking rows, as we found a match\n",
    "\n",
    "        # If no matching row was found, add the combination to output_df\n",
    "        if not combination_in_input_df:\n",
    "            combination[\"model\"] = model\n",
    "            combination[\"dataset\"] = dataset\n",
    "            output_df = pd.concat([output_df, pd.DataFrame([combination])], ignore_index=True)\n",
    "            del combination[\"model\"]\n",
    "            del combination[\"dataset\"]\n",
    "\n",
    "    return output_df\n",
    "\n",
    "# Create an empty dataframe to store missing combinations\n",
    "missing_comb_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset in datasets:\n",
    "    # Select the relevant model names based on the dataset\n",
    "    if \"kitti\" in dataset:\n",
    "        model_names = kitti_model_names\n",
    "    else:\n",
    "        model_names = sintel_model_names\n",
    "    \n",
    "    # Iterate through each model in the dataset\n",
    "    for model in model_names:\n",
    "        # Filter the dataframe for the current dataset and model\n",
    "        subset_df = merged_df[(merged_df['dataset'] == dataset) & (merged_df['model'] == model)]\n",
    "        missing_comb_df = combination_matches_row(subset_df, combinations_list, missing_comb_df, model, dataset)\n",
    "\n",
    "# Rearrange columns to have 'model' and 'dataset' first\n",
    "columns_order = ['model', 'dataset'] + [col for col in missing_comb_df.columns if col not in ['model', 'dataset']]\n",
    "missing_comb_df = missing_comb_df[columns_order]\n",
    "\n",
    "# Sort by 'model', 'dataset', 'norm', and 'attack'\n",
    "if 'norm' in missing_comb_df.columns:\n",
    "    missing_comb_df = missing_comb_df.sort_values(by=['model', 'dataset','norm', 'attack'], ascending=True)\n",
    "else:\n",
    "    missing_comb_df = missing_comb_df.sort_values(by=['model', 'dataset', 'attack'], ascending=True)\n",
    "\n",
    "# Save the missing combinations dataframe to a CSV file\n",
    "missing_comb_df.to_csv('missing_combinations.csv', index=False)\n",
    "\n",
    "print(\"Missing combinations saved to 'missing_combinations.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "combinations_list = [\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'pgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': True, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.0314, target: 'negative' or 'zero' (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0314, 'target': 'zero', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'negative', 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'zero', 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: True, epsilon: 0.2510, target: 'negative' or 'zero' (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.2510, 'target': 'zero', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'negative', 'norm': 'two'},\n",
    "    {'attack': 'fgsm', 'targeted': True, 'loss': 'epe', 'epsilon': 0.0, 'target': 'zero', 'norm': 'two'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attacks: pgd, cospgd, bim, norm: 'inf')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.01, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attacks: pgd, cospgd, bim, norm: 'two')\n",
    "    {'attack': 'pgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'cospgd', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "    {'attack': 'bim', 'targeted': False, 'iterations': 20.0, 'alpha': 0.1, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    #Top 9 models configuration\n",
    "    {'targeted': False, 'attack': 'cospgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "    {'targeted': False, 'attack': 'pgd', 'epsilon': 0.0314, 'alpha': 0.01, 'iterations': 20.0, 'norm': 'inf', 'loss': 'epe', 'optim': \"initial_flow\"},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.0314 or 0.0157, target: None (for attack: fgsm, norm: 'inf')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0314, 'norm': 'inf'},\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.0157, 'norm': 'inf'},\n",
    "\n",
    "    # Targeted: False, epsilon: 0.2510, target: None (for attack: fgsm, norm: 'two')\n",
    "    {'attack': 'fgsm', 'targeted': False, 'loss': 'epe', 'epsilon': 0.2510, 'norm': 'two'},\n",
    "\n",
    "    {'attack': 'common_corruptions', 'name': 'gaussian_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'shot_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'impulse_noise', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'defocus_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'glass_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'motion_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'zoom_blur', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'snow', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'frost', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'fog', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'brightness', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'contrast', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'elastic_transform', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'pixelate', 'severity': 3.0},\n",
    "    {'attack': 'common_corruptions', 'name': 'jpeg_compression', 'severity': 3.0},\n",
    "\n",
    "    {'attack': 'weather', 'targeted': False, 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative', 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'zero', 'flakesize': 71.0, 'depth': 9.0, 'transparency': 0.75, 'do': 0.0, 'weather_type': 'snow'},\n",
    "\n",
    "    {'attack': 'weather', 'targeted': False, 'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0,  'weather_type': 'rain'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0, 'weather_type': 'rain'},\n",
    "    {'attack': 'weather', 'targeted': True,'target': 'zero',  'num': 1000.0, 'flakesize': 51.0, 'depth': 9.0, 'transparency': 0.75, 'do': 1.0, 'weather_type': 'rain'},\n",
    "\n",
    "    {'attack': 'weather', 'targeted': False, 'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0,  'weather_type': 'fog'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0, 'weather_type': 'fog'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'zero',  'num': 60.0, 'flakesize': 451.0, 'depth': 0.8, 'constant': 0.3, 'transparency': 0.25, 'do': 0.0, 'weather_type': 'fog'},\n",
    "    \n",
    "    {'attack': 'weather', 'targeted': False, 'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0,  'weather_type': 'sparks'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'negative',  'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0, 'weather_type': 'sparks'},\n",
    "    {'attack': 'weather', 'targeted': True, 'target': 'zero',  'num': 3000.0, 'flakesize': 41.0, 'depth': 9.0, 'transparency': 1.5, 'do': 1.0, 'weather_type': 'sparks'},\n",
    "\n",
    "    {'attack': 'none'},\n",
    "\n",
    "]\n",
    "\n",
    "# Set the dataset and norm\n",
    "datasets = [\"sintel-final\", \"sintel-clean\", \"kitti-2015\"]\n",
    "\n",
    "# Define the datasets and corresponding model names\n",
    "kitti_model_names = [\n",
    "    \"raft\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"csflow\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"flow1d\", \"flowformer\", \"flowformer++\", \n",
    "    \"gmflow\", \"gmflownet\", \"hd3\", \"irr_pwc\", \"liteflownet\", \n",
    "    \"liteflownet3_pseudoreg\", \"llaflow\", \"ms_raft+\", \n",
    "    \"rapidflow\", \"scopeflow\", \"skflow\", \n",
    "    \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "sintel_model_names = [\n",
    "    \"raft\", \"pwcnet\", \"gma\", \"rpknet\", \"ccmr\", \"craft\", \"dicl\", \"dip\", \n",
    "    \"fastflownet\", \"maskflownet\", \"maskflownet_s\", \"flow1d\", \"flowformer\", \n",
    "    \"flowformer++\", \"gmflow\", \"hd3\", \"liteflownet\", \n",
    "    \"liteflownet2\", \"liteflownet3\", \"llaflow\", \n",
    "    \"ms_raft+\", \"rapidflow\", \"scopeflow\", \n",
    "    \"skflow\", \"starflow\", \"videoflow_bof\"\n",
    "]\n",
    "\n",
    "kitti_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3_pseudoreg\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "sintel_model_names_top_9 = [\n",
    "    \"raft\", \"rpknet\", \"craft\", \"maskflownet\", \"flow1d\", \"flowformer++\", \n",
    "    \"liteflownet3\", \"ms_raft+\", \n",
    "    \"scopeflow\", \"gma\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the casting dictionary\n",
    "attributes_to_cast = {\n",
    "    'attack': str,\n",
    "    'targeted': 'boolean',  # Can use a boolean cast, True/False or None\n",
    "    'iterations': float,\n",
    "    'alpha': float,\n",
    "    'loss': str,\n",
    "    'epsilon': float,\n",
    "    'target': str,\n",
    "    'norm': str,\n",
    "    'name': str,\n",
    "    'severity': float\n",
    "}\n",
    "\n",
    "# Function to cast columns in merged_df\n",
    "def cast_columns(merged_df, attributes_to_cast):\n",
    "    for col, dtype in attributes_to_cast.items():\n",
    "        if dtype == 'boolean':\n",
    "            merged_df[col] = merged_df[col].astype('bool', errors='ignore')\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].astype(dtype, errors='ignore')\n",
    "    return merged_df\n",
    "\n",
    "# Cast the columns\n",
    "merged_df = cast_columns(merged_df, attributes_to_cast)\n",
    "\n",
    "def filter_and_append_rows_by_combination(input_df, combination_list, output_df):\n",
    "    # Ensure matching_rows_mask is the same length as the input_df\n",
    "    matching_rows_mask = pd.Series([False] * len(input_df), index=input_df.index)\n",
    "\n",
    "    # Iterate over each combination in the combination list\n",
    "    for combination in combination_list:\n",
    "        # Create a mask to identify rows that match the current combination\n",
    "        combination_mask = pd.Series([True] * len(input_df), index=input_df.index)\n",
    "\n",
    "        # Loop through each key-value pair in the combination\n",
    "        for key, value in combination.items():\n",
    "            if key in input_df.columns:\n",
    "                # Update the mask for the current combination\n",
    "                combination_mask &= (input_df[key] == value)\n",
    "            else:\n",
    "                # If a key in the combination is not in the input_df, skip this combination\n",
    "                combination_mask &= False\n",
    "\n",
    "        # Update the overall matching rows mask\n",
    "        matching_rows_mask |= combination_mask\n",
    "\n",
    "    # Filter the input_df to keep only the matching rows\n",
    "    filtered_df = input_df[matching_rows_mask].reset_index(drop=True)\n",
    "\n",
    "    # Concatenate filtered_df with output_df\n",
    "    output_df = pd.concat([output_df, filtered_df], ignore_index=True)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty dataframe to store missing combinations\n",
    "relevant_experiments = pd.DataFrame()\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset in datasets:\n",
    "    # Select the relevant model names based on the dataset\n",
    "    if \"kitti\" in dataset:\n",
    "        model_names = kitti_model_names\n",
    "    else:\n",
    "        model_names = sintel_model_names\n",
    "    \n",
    "    # Iterate through each model in the dataset\n",
    "    for model in model_names:\n",
    "        # Filter the dataframe for the current dataset and model\n",
    "        subset_df = merged_df[(merged_df['dataset'] == dataset) & (merged_df['model'] == model)]\n",
    "        relevant_experiments = filter_and_append_rows_by_combination(subset_df, combinations_list, relevant_experiments)\n",
    "\n",
    "# Rearrange columns to have 'model' and 'dataset' first\n",
    "columns_order = ['model', 'dataset'] + [col for col in relevant_experiments.columns if col not in ['model', 'dataset']]\n",
    "relevant_experiments = relevant_experiments[columns_order]\n",
    "\n",
    "# Sort by 'model', 'dataset', 'norm', and 'attack'\n",
    "relevant_experiments = relevant_experiments.sort_values(by=['model', 'dataset', 'norm', 'attack'], ascending=True)\n",
    "\n",
    "with open('model_params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into a DataFrame\n",
    "df_params = pd.DataFrame(data['experiment'])\n",
    "\n",
    "final_df = pd.merge(left=relevant_experiments, right=df_params, on=\"model\")\n",
    "\n",
    "# Save the missing combinations dataframe to a CSV file\n",
    "final_df.to_csv('existing_relevant_combinations.csv', index=False)\n",
    "\n",
    "print(\"Relevant combinations saved to 'existing_relevant_combinations.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
