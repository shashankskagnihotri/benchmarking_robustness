{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fgsm import perform_fgsm_attack\n",
    "from dataloader import get_dataset\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from datasets import __datasets__\n",
    "from models import __models__, model_loss\n",
    "from utils import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import gc\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataloader import get_dataset\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation of:\n",
    "Agnihotri, Shashank, Jung, Steffen, Keuper, Margret. \"CosPGD: a unified white-box adversarial attack for pixel-wise prediction tasks.\" \n",
    "arXiv preprint arXiv:2302.02213 (2023).\n",
    "\n",
    "A tool for benchmarking adversarial robustness of pixel-wise prediction tasks.\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2023 Shashank Agnihotri, Steffen Jung, Prof. Dr. Margret Keuper\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "class Attack:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to take one attack step in the l-infinity norm constraint\n",
    "\n",
    "    perturbed_image: Float tensor of shape [batch size, channels, (image spatial resolution)]\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    data_grad: gradient on the image input to the model w.r.t. the loss backpropagated\n",
    "    orig_image: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise\n",
    "    alpha: Float tensor: attack step size\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space\n",
    "    grad_scale: tensor either single value or of the same shape as data_grad: to scale the added noise\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def step_inf(\n",
    "            perturbed_image,\n",
    "            epsilon,\n",
    "            data_grad,\n",
    "            orig_image,\n",
    "            alpha,\n",
    "            targeted,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "            grad_scale = None\n",
    "        ):\n",
    "        # Collect the element-wise sign of the data gradient\n",
    "        sign_data_grad = alpha*data_grad.sign()\n",
    "        if targeted:\n",
    "            sign_data_grad *= -1\n",
    "        if grad_scale is not None:\n",
    "            sign_data_grad *= grad_scale\n",
    "        # Create the perturbed image by adjusting each pixel of the input image\n",
    "        perturbed_image = perturbed_image.detach() + sign_data_grad\n",
    "        # Adding clipping to maintain [0,1] range\n",
    "        delta = torch.clamp(perturbed_image - orig_image, min=-epsilon, max=epsilon)\n",
    "        perturbed_image = torch.clamp(orig_image + delta, clamp_min, clamp_max).detach()\n",
    "        return perturbed_image\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to take one attack step in the l2 norm constraint\n",
    "    \n",
    "    perturbed_image: Float tensor of shape [batch size, channels, (image spatial resolution)]\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    data_grad: gradient on the image input to the model w.r.t. the loss backpropagated\n",
    "    orig_image: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise\n",
    "    alpha: Float tensor: attack step size\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space\n",
    "    grad_scale: tensor either single value or of the same shape as data_grad: to scale the added noise\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def step_l2(\n",
    "            perturbed_image,\n",
    "            epsilon,\n",
    "            data_grad,\n",
    "            orig_image,\n",
    "            alpha,\n",
    "            targeted,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "            grad_scale = None\n",
    "        ):\n",
    "        # normalize gradients\n",
    "        if targeted:\n",
    "            data_grad *= -1\n",
    "        data_grad = Attack.lp_normalize(\n",
    "            data_grad,\n",
    "            p = 2,\n",
    "            epsilon = 1.0,\n",
    "            decrease_only = False\n",
    "        )\n",
    "        if grad_scale is not None:\n",
    "            data_grad *= grad_scale\n",
    "        # Create the perturbed image by adjusting each pixel of the input image\n",
    "        perturbed_image = perturbed_image.detach() + alpha*data_grad\n",
    "        # clip to l2 ball\n",
    "        delta = Attack.lp_normalize(\n",
    "            noise = perturbed_image - orig_image,\n",
    "            p = 2,\n",
    "            epsilon = epsilon,\n",
    "            decrease_only = True\n",
    "        )\n",
    "        # Adding clipping to maintain [0,1] range\n",
    "        perturbed_image = torch.clamp(orig_image + delta, clamp_min, clamp_max).detach()\n",
    "        return perturbed_image\n",
    "    \n",
    "    \"\"\"\n",
    "    Clamping noise in the l-p norm constraint\n",
    "    noise: tensor of shape [batch size, (image spatial resolution)]: the noise to be clamped\n",
    "    p: int: the norm\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    decrease_only: boolean: to only clamp the upper bound and not the lower bound\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def lp_normalize(\n",
    "            noise,\n",
    "            p,\n",
    "            epsilon = None,\n",
    "            decrease_only = False\n",
    "        ):\n",
    "        if epsilon is None:\n",
    "            epsilon = torch.tensor(1.0)\n",
    "        denom = torch.norm(noise, p=p, dim=(-1, -2, -3))\n",
    "        denom = torch.maximum(denom, torch.tensor(1E-12)).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        if decrease_only:\n",
    "            denom = torch.maximum(denom/epsilon, torch.tensor(1))\n",
    "        else:\n",
    "            denom = denom / epsilon\n",
    "        return noise / denom\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing noise in the l-infinity norm constraint\n",
    "\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    images: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise    \n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def init_linf(\n",
    "            images,\n",
    "            epsilon,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "        ):\n",
    "        noise = torch.FloatTensor(images.shape).uniform_(-epsilon, epsilon).to(images.device)\n",
    "        images = images + noise\n",
    "        images = images.clamp(clamp_min, clamp_max)\n",
    "        return images\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing noise in the l-2 norm constraint\n",
    "\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    images: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise    \n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def init_l2(\n",
    "            images,\n",
    "            epsilon,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "        ):\n",
    "        noise = torch.FloatTensor(images.shape).uniform_(-1, 1).to(images.device)\n",
    "        noise = Attack.lp_normalize(\n",
    "            noise = noise,\n",
    "            p = 2,\n",
    "            epsilon = epsilon,\n",
    "            decrease_only = False\n",
    "        )\n",
    "        images = images + noise\n",
    "        images = images.clamp(clamp_min, clamp_max)\n",
    "        return images\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Scaling of the pixel-wise loss as proposed by: \n",
    "    Gu, Jindong, et al. \"Segpgd: An effective and efficient adversarial attack for evaluating and boosting segmentation robustness.\" \n",
    "    European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n",
    "\n",
    "    predictions: Float tensor of shape [batch size, channel, (image spatial resolution)]: Predictions made by the model\n",
    "    labels: The ground truth/target labels, for semantic segmentation index tensor of the shape: [batch size, channel, (image spatial resolution)].\n",
    "                                     for pixel-wise regression tasks, same shape as predictions\n",
    "    loss: Float tensor: The loss between the predictions and the ground truth/target\n",
    "    iteration: Current attack iteration for calculating lambda as used in SegPGD\n",
    "    iterations: Total number of attack iterations for calculating lambda as used in SegPGD\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def segpgd_scale(\n",
    "            predictions,\n",
    "            labels,\n",
    "            loss,\n",
    "            iteration,\n",
    "            iterations,\n",
    "            targeted=False,\n",
    "        ):\n",
    "        lambda_t = iteration/(2*iterations)\n",
    "        output_idx = torch.argmax(predictions, dim=1)\n",
    "        if targeted:\n",
    "            loss = torch.sum(\n",
    "                torch.where(\n",
    "                    output_idx == labels,\n",
    "                    lambda_t*loss,\n",
    "                    (1-lambda_t)*loss\n",
    "                )\n",
    "            ) / (predictions.shape[-2]*predictions.shape[-1])\n",
    "        else:\n",
    "            loss = torch.sum(\n",
    "                torch.where(\n",
    "                    output_idx == labels,\n",
    "                    (1-lambda_t)*loss,\n",
    "                    lambda_t*loss\n",
    "                )\n",
    "            ) / (predictions.shape[-2]*predictions.shape[-1])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Scaling of the pixel-wise loss as implemeted by: \n",
    "    Agnihotri, Shashank, et al. \"CosPGD: a unified white-box adversarial attack for pixel-wise prediction tasks.\" \n",
    "    arXiv preprint arXiv:2302.02213 (2023).\n",
    "\n",
    "    predictions: Float tensor of shape [batch size, channel, (image spatial resolution)]: Predictions made by the model\n",
    "    labels: The ground truth/target labels, for semantic segmentation index tensor of the shape: [batch size, channel, (image spatial resolution)].\n",
    "                                     for pixel-wise regression tasks, same shape as predictions\n",
    "    loss: Float tensor: The loss between the predictions and the ground truth/target\n",
    "    num_classes: int: For semantic segmentation the number of classes. None for pixel-wise regression tasks\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    one_hot: boolean: To use one-hot encoding, SHOULD BE TRUE FOR SEMANTIC SEGMENTATION and FALSE FOR pixel-wise regression tasks\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def cospgd_scale(\n",
    "            predictions,\n",
    "            labels,\n",
    "            loss,\n",
    "            num_classes=None,\n",
    "            targeted=False,\n",
    "            one_hot=True,\n",
    "        ):\n",
    "        if one_hot:\n",
    "            transformed_target = torch.nn.functional.one_hot(\n",
    "                torch.clamp(labels, labels.min(), num_classes-1),\n",
    "                num_classes = num_classes\n",
    "            ).permute(0,3,1,2)\n",
    "        else:\n",
    "            transformed_target = torch.nn.functional.softmax(labels, dim=1)\n",
    "        cossim = torch.nn.functional.cosine_similarity(\n",
    "            torch.nn.functional.softmax(predictions, dim=1),\n",
    "            transformed_target,\n",
    "            dim = 1\n",
    "        )\n",
    "        if targeted:\n",
    "            cossim = 1 - cossim # if performing targeted attacks, we want to punish for dissimilarity to the target\n",
    "        loss = cossim.detach() * loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Implementation of:\n",
    "Agnihotri, Shashank, Jung, Steffen, Keuper, Margret. \"CosPGD: a unified white-box adversarial attack for pixel-wise prediction tasks.\" \n",
    "arXiv preprint arXiv:2302.02213 (2023).\n",
    "\n",
    "A tool for benchmarking adversarial robustness of pixel-wise prediction tasks.\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2023 Shashank Agnihotri, Steffen Jung, Prof. Dr. Margret Keuper\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "class Attack:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to take one attack step in the l-infinity norm constraint\n",
    "\n",
    "    perturbed_image: Float tensor of shape [batch size, channels, (image spatial resolution)]\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    data_grad: gradient on the image input to the model w.r.t. the loss backpropagated\n",
    "    orig_image: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise\n",
    "    alpha: Float tensor: attack step size\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space\n",
    "    grad_scale: tensor either single value or of the same shape as data_grad: to scale the added noise\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def step_inf(\n",
    "            perturbed_image,\n",
    "            epsilon,\n",
    "            data_grad,\n",
    "            orig_image,\n",
    "            alpha,\n",
    "            targeted,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "            grad_scale = None\n",
    "        ):\n",
    "        # Collect the element-wise sign of the data gradient\n",
    "        sign_data_grad = alpha*data_grad.sign()\n",
    "        if targeted:\n",
    "            sign_data_grad *= -1\n",
    "        if grad_scale is not None:\n",
    "            sign_data_grad *= grad_scale\n",
    "        # Create the perturbed image by adjusting each pixel of the input image\n",
    "        perturbed_image = perturbed_image.detach() + sign_data_grad\n",
    "        # Adding clipping to maintain [0,1] range\n",
    "        delta = torch.clamp(perturbed_image - orig_image, min=-epsilon, max=epsilon)\n",
    "        perturbed_image = torch.clamp(orig_image + delta, clamp_min, clamp_max).detach()\n",
    "        return perturbed_image\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to take one attack step in the l2 norm constraint\n",
    "    \n",
    "    perturbed_image: Float tensor of shape [batch size, channels, (image spatial resolution)]\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    data_grad: gradient on the image input to the model w.r.t. the loss backpropagated\n",
    "    orig_image: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise\n",
    "    alpha: Float tensor: attack step size\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space\n",
    "    grad_scale: tensor either single value or of the same shape as data_grad: to scale the added noise\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def step_l2(\n",
    "            perturbed_image,\n",
    "            epsilon,\n",
    "            data_grad,\n",
    "            orig_image,\n",
    "            alpha,\n",
    "            targeted,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "            grad_scale = None\n",
    "        ):\n",
    "        # normalize gradients\n",
    "        if targeted:\n",
    "            data_grad *= -1\n",
    "        data_grad = Attack.lp_normalize(\n",
    "            data_grad,\n",
    "            p = 2,\n",
    "            epsilon = 1.0,\n",
    "            decrease_only = False\n",
    "        )\n",
    "        if grad_scale is not None:\n",
    "            data_grad *= grad_scale\n",
    "        # Create the perturbed image by adjusting each pixel of the input image\n",
    "        perturbed_image = perturbed_image.detach() + alpha*data_grad\n",
    "        # clip to l2 ball\n",
    "        delta = Attack.lp_normalize(\n",
    "            noise = perturbed_image - orig_image,\n",
    "            p = 2,\n",
    "            epsilon = epsilon,\n",
    "            decrease_only = True\n",
    "        )\n",
    "        # Adding clipping to maintain [0,1] range\n",
    "        perturbed_image = torch.clamp(orig_image + delta, clamp_min, clamp_max).detach()\n",
    "        return perturbed_image\n",
    "    \n",
    "    \"\"\"\n",
    "    Clamping noise in the l-p norm constraint\n",
    "    noise: tensor of shape [batch size, (image spatial resolution)]: the noise to be clamped\n",
    "    p: int: the norm\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    decrease_only: boolean: to only clamp the upper bound and not the lower bound\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def lp_normalize(\n",
    "            noise,\n",
    "            p,\n",
    "            epsilon = None,\n",
    "            decrease_only = False\n",
    "        ):\n",
    "        if epsilon is None:\n",
    "            epsilon = torch.tensor(1.0)\n",
    "        denom = torch.norm(noise, p=p, dim=(-1, -2, -3))\n",
    "        denom = torch.maximum(denom, torch.tensor(1E-12)).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        if decrease_only:\n",
    "            denom = torch.maximum(denom/epsilon, torch.tensor(1))\n",
    "        else:\n",
    "            denom = denom / epsilon\n",
    "        return noise / denom\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing noise in the l-infinity norm constraint\n",
    "\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    images: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise    \n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def init_linf(\n",
    "            images,\n",
    "            epsilon,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "        ):\n",
    "        noise = torch.FloatTensor(images.shape).uniform_(-epsilon, epsilon).to(images.device)\n",
    "        images = images + noise\n",
    "        images = images.clamp(clamp_min, clamp_max)\n",
    "        return images\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing noise in the l-2 norm constraint\n",
    "\n",
    "    epsilon: Float tensor: permissible epsilon range\n",
    "    images: Float tensor of shape [batch size, channels, (image spatial resolution)]: Original unattacked image, before adding any noise    \n",
    "    clamp_min: Float tensor: minimum clip value for clipping the perturbed image back to the permisible input space\n",
    "    clamp_max: Float tensor: maximum clip value for clipping the perturbed image back to the permisible input space\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def init_l2(\n",
    "            images,\n",
    "            epsilon,\n",
    "            clamp_min = 0,\n",
    "            clamp_max = 1,\n",
    "        ):\n",
    "        noise = torch.FloatTensor(images.shape).uniform_(-1, 1).to(images.device)\n",
    "        noise = Attack.lp_normalize(\n",
    "            noise = noise,\n",
    "            p = 2,\n",
    "            epsilon = epsilon,\n",
    "            decrease_only = False\n",
    "        )\n",
    "        images = images + noise\n",
    "        images = images.clamp(clamp_min, clamp_max)\n",
    "        return images\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Scaling of the pixel-wise loss as proposed by: \n",
    "    Gu, Jindong, et al. \"Segpgd: An effective and efficient adversarial attack for evaluating and boosting segmentation robustness.\" \n",
    "    European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n",
    "\n",
    "    predictions: Float tensor of shape [batch size, channel, (image spatial resolution)]: Predictions made by the model\n",
    "    labels: The ground truth/target labels, for semantic segmentation index tensor of the shape: [batch size, channel, (image spatial resolution)].\n",
    "                                     for pixel-wise regression tasks, same shape as predictions\n",
    "    loss: Float tensor: The loss between the predictions and the ground truth/target\n",
    "    iteration: Current attack iteration for calculating lambda as used in SegPGD\n",
    "    iterations: Total number of attack iterations for calculating lambda as used in SegPGD\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def segpgd_scale(\n",
    "            predictions,\n",
    "            labels,\n",
    "            loss,\n",
    "            iteration,\n",
    "            iterations,\n",
    "            targeted=False,\n",
    "        ):\n",
    "        lambda_t = iteration/(2*iterations)\n",
    "        output_idx = torch.argmax(predictions, dim=1)\n",
    "        if targeted:\n",
    "            loss = torch.sum(\n",
    "                torch.where(\n",
    "                    output_idx == labels,\n",
    "                    lambda_t*loss,\n",
    "                    (1-lambda_t)*loss\n",
    "                )\n",
    "            ) / (predictions.shape[-2]*predictions.shape[-1])\n",
    "        else:\n",
    "            loss = torch.sum(\n",
    "                torch.where(\n",
    "                    output_idx == labels,\n",
    "                    (1-lambda_t)*loss,\n",
    "                    lambda_t*loss\n",
    "                )\n",
    "            ) / (predictions.shape[-2]*predictions.shape[-1])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Scaling of the pixel-wise loss as implemeted by: \n",
    "    Agnihotri, Shashank, et al. \"CosPGD: a unified white-box adversarial attack for pixel-wise prediction tasks.\" \n",
    "    arXiv preprint arXiv:2302.02213 (2023).\n",
    "\n",
    "    predictions: Float tensor of shape [batch size, channel, (image spatial resolution)]: Predictions made by the model\n",
    "    labels: The ground truth/target labels, for semantic segmentation index tensor of the shape: [batch size, channel, (image spatial resolution)].\n",
    "                                     for pixel-wise regression tasks, same shape as predictions\n",
    "    loss: Float tensor: The loss between the predictions and the ground truth/target\n",
    "    num_classes: int: For semantic segmentation the number of classes. None for pixel-wise regression tasks\n",
    "    targeted: boolean: Targeted attack or not\n",
    "    one_hot: boolean: To use one-hot encoding, SHOULD BE TRUE FOR SEMANTIC SEGMENTATION and FALSE FOR pixel-wise regression tasks\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def cospgd_scale(\n",
    "            predictions,\n",
    "            labels,\n",
    "            loss,\n",
    "            num_classes=None,\n",
    "            targeted=False,\n",
    "            one_hot=True,\n",
    "        ):\n",
    "        if one_hot:\n",
    "            transformed_target = torch.nn.functional.one_hot(\n",
    "                torch.clamp(labels, labels.min(), num_classes-1),\n",
    "                num_classes = num_classes\n",
    "            ).permute(0,3,1,2)\n",
    "        else:\n",
    "            transformed_target = torch.nn.functional.softmax(labels, dim=1)\n",
    "        cossim = torch.nn.functional.cosine_similarity(\n",
    "            torch.nn.functional.softmax(predictions, dim=1),\n",
    "            transformed_target,\n",
    "            dim = 1\n",
    "        )\n",
    "        if targeted:\n",
    "            cossim = 1 - cossim # if performing targeted attacks, we want to punish for dissimilarity to the target\n",
    "        loss = cossim.detach() * loss\n",
    "        return loss\n",
    "\n",
    "class CosPGDAttack:\n",
    "    def __init__(self, model, epsilon, alpha, num_iterations, num_classes=None, targeted=False):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_classes = num_classes\n",
    "        self.targeted = targeted\n",
    "    \n",
    "    def attack(self, left_image, right_image, labels):\n",
    "        # Initialize perturbations for both left and right images\n",
    "        perturbed_left = Attack.init_linf(left_image, self.epsilon)\n",
    "        perturbed_right = Attack.init_linf(right_image, self.epsilon)\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            perturbed_left.requires_grad = True\n",
    "            perturbed_right.requires_grad = True\n",
    "            \n",
    "            # Forward pass the perturbed images through the model\n",
    "            outputs = self.model(perturbed_left, perturbed_right)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = F.mse_loss(outputs, labels)\n",
    "            \n",
    "            # Zero all existing gradients\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            # Backward pass to compute gradients of the loss w.r.t the perturbed images\n",
    "            loss.backward()\n",
    "            \n",
    "            # Collect the gradient data\n",
    "            left_grad = perturbed_left.grad.data\n",
    "            right_grad = perturbed_right.grad.data\n",
    "            \n",
    "            # Perform the attack step\n",
    "            perturbed_left = Attack.step_inf(\n",
    "                perturbed_image=perturbed_left,\n",
    "                epsilon=self.epsilon,\n",
    "                data_grad=left_grad,\n",
    "                orig_image=left_image,\n",
    "                alpha=self.alpha,\n",
    "                targeted=self.targeted,\n",
    "                clamp_min=0,\n",
    "                clamp_max=1\n",
    "            )\n",
    "            \n",
    "            perturbed_right = Attack.step_inf(\n",
    "                perturbed_image=perturbed_right,\n",
    "                epsilon=self.epsilon,\n",
    "                data_grad=right_grad,\n",
    "                orig_image=right_image,\n",
    "                alpha=self.alpha,\n",
    "                targeted=self.targeted,\n",
    "                clamp_min=0,\n",
    "                clamp_max=1\n",
    "            )\n",
    "        \n",
    "        return perturbed_left, perturbed_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = '/pfs/work7/workspace/scratch/ma_faroesch-team_project_fss2024/dataset/FlyingThings3D'\n",
    "# train_dataset = get_dataset(\"sceneflow\", dataset_path, architeture_name=\"CFNet\", split='test') # maybe test -> test daten \n",
    "\n",
    "# model = __models__[\"cfnet\"](284)\n",
    "# model = nn.DataParallel(model)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "# best_val_loss = -1\n",
    "\n",
    "# # load parameters\n",
    "# start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_left  = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# image_right = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# attack_epsilon = 0.03  # Epsilon value for the FGSM attack\n",
    "# from torchvision import transforms\n",
    "# transform = transforms.ToTensor()\n",
    "# tensor_image_left= transform(image_left)\n",
    "# tensor_image_left= torch.unsqueeze(tensor_image_left, 0)\n",
    "# tensor_image_right= transform(image_right)\n",
    "# tensor_image_righ= torch.unsqueeze(tensor_image_right, 0)\n",
    "# preds, perturbed_inputs = perform_fgsm_attack(model, tensor_image_left ,tensor_image_right , attack_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_left  = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# image_right = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# attack_epsilon = 0.03  # Epsilon value for the FGSM attack\n",
    "# from torchvision import transforms\n",
    "# transform = transforms.ToTensor()\n",
    "# preds, perturbed_inputs = perform_fgsm_attack(model, transform(image_left), transform(image_right), attack_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading kitti2015 dataset\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KITTIBaseDataset' object has no attribute 'img_left_filenames'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pfs/work7/workspace/scratch/ma_faroesch-team_project_fss2024/dataset/KITTI_2015\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkitti2015\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_path, architeture_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCFNet\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# maybe test -> test daten \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m image_left  \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mload_image(train_dataset\u001b[38;5;241m.\u001b[39mimg_left_filenames[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m image_right \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mload_image(train_dataset\u001b[38;5;241m.\u001b[39mimg_left_filenames[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KITTIBaseDataset' object has no attribute 'img_left_filenames'"
     ]
    }
   ],
   "source": [
    "# # load images\n",
    "# dataset_path = '/pfs/work7/workspace/scratch/ma_faroesch-team_project_fss2024/dataset/KITTI_2015'\n",
    "# train_dataset = get_dataset(\"kitti2015\", dataset_path, architeture_name=\"CFNet\", split='train') # maybe test -> test daten \n",
    "# image_left  = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# image_right = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# transform = transforms.ToTensor()\n",
    "# image_left = transform(image_left)\n",
    "# image_right = transform(image_right)\n",
    "# print(image_left)\n",
    "# disparity = train_dataset.load_disp(train_dataset.disp_left_filenames[0])\n",
    "# disparity = transform(disparity)\n",
    "\n",
    "# model = __models__[\"cfnet\"](284)\n",
    "# model = nn.DataParallel(model)\n",
    "\n",
    "# # disparity = disparity.unsqueeze(0) # how to get the labels ???? \n",
    "\n",
    "# epsilon = 0.03\n",
    "# alpha = 0.01\n",
    "# num_iterations = 10\n",
    "\n",
    "# attacker = CosPGDAttack(model, epsilon, alpha, num_iterations, num_classes=None, targeted=False)\n",
    "# perturbed_left_image, perturbed_right_image = attacker.attack(image_left.unsqueeze(0), image_right.unsqueeze(0), disparity.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import cfnet, sttr, sttr_light, psmnet, hsmnet, gwcnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sceneflow dataset\n",
      "torch.Size([3, 512, 960])\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m attacker \u001b[38;5;241m=\u001b[39m CosPGDAttack(model, epsilon, alpha, num_iterations, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, targeted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# perturbed_left_image, perturbed_right_image = attacker.attack(image_left, image_right, disparity)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m perturbed_left_image, perturbed_right_image \u001b[38;5;241m=\u001b[39m attacker\u001b[38;5;241m.\u001b[39mattack(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisparity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 302\u001b[0m, in \u001b[0;36mCosPGDAttack.attack\u001b[0;34m(self, left_image, right_image, labels)\u001b[0m\n\u001b[1;32m    299\u001b[0m perturbed_right\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Forward pass the perturbed images through the model\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(perturbed_left, perturbed_right)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m    305\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(outputs, labels)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:543\u001b[0m, in \u001b[0;36mcfnet.forward\u001b[0;34m(self, left, right)\u001b[0m\n\u001b[1;32m    541\u001b[0m cost0_6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdres0_6(volume6)\n\u001b[1;32m    542\u001b[0m cost0_6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdres1_6(cost0_6) \u001b[38;5;241m+\u001b[39m cost0_6\n\u001b[0;32m--> 543\u001b[0m out1_4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine1(cost0_4, cost0_5, cost0_6)\n\u001b[1;32m    544\u001b[0m out2_4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdres3(out1_4)\n\u001b[1;32m    547\u001b[0m cost2_s4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassif2(out2_4)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:228\u001b[0m, in \u001b[0;36mhourglassup.forward\u001b[0;34m(self, x, feature4, feature5)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, feature4, feature5):\n\u001b[1;32m    227\u001b[0m     conv1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)          \u001b[38;5;66;03m#1/8\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     conv1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((conv1, feature4), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m#1/8\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     conv1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine1(conv1)   \u001b[38;5;66;03m#1/8\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     conv2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(conv1)      \u001b[38;5;66;03m#1/8\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "# load images\n",
    "dataset_path = '/pfs/work7/workspace/scratch/ma_faroesch-team_project_fss2024/dataset/FlyingThings3D'\n",
    "train_dataset = get_dataset(\"sceneflow\", dataset_path, architeture_name=\"CFNet\", split='test') # maybe test -> test daten \n",
    "batch = next(iter(train_dataset))\n",
    "\n",
    "# image_left  = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# image_right = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "# transform = transforms.ToTensor()\n",
    "# image_left = transform(image_left)\n",
    "# image_right = transform(image_right)\n",
    "# print(image_left.size())\n",
    "# print(image_right.size())\n",
    "# disparity = train_dataset.load_disp(train_dataset.disp_left_filenames[0])\n",
    "\n",
    "# disparity = transform(disparity)\n",
    "# th, tw = 256, 512\n",
    "# print(cfnet.flow_transforms.RandomCrop((th, tw)).size)\n",
    "# co_transform =  cfnet.flow_transforms.Compose([cfnet.flow_transforms.RandomCrop((th, tw)),\n",
    "#         ])\n",
    "# augmented, disparity = co_transform([image_left, image_right], disparity)\n",
    "# image_left = augmented[0]\n",
    "# image_right = augmented[1]\n",
    "# print(image_left.size())\n",
    "# print(image_right.size())\n",
    "print(batch['left'].size())\n",
    "\n",
    "\n",
    "\n",
    "# disparity = train_dataset.load_disp(train_dataset.disp_left_filenames[0])\n",
    "# disparity = transform(disparity)\n",
    "\n",
    "model = __models__[\"cfnet\"](284)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "\n",
    "epsilon = 0.03\n",
    "alpha = 0.01\n",
    "num_iterations = 10\n",
    "\n",
    "attacker = CosPGDAttack(model, epsilon, alpha, num_iterations, num_classes=None, targeted=False)\n",
    "# perturbed_left_image, perturbed_right_image = attacker.attack(image_left, image_right, disparity)\n",
    "perturbed_left_image, perturbed_right_image = attacker.attack(batch['left'].unsqueeze(0), batch['right'].unsqueeze(0), batch['disparity'].unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for loop einbauen - um verschiedene schritte zu machen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sceneflow dataset\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n",
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/pfs/work7/workspace/scratch/ma_faroesch-team_project_fss2024/dataset/FlyingThings3D'\n",
    "train_dataset = get_dataset(\"sceneflow\", dataset_path, architeture_name=\"CFNet\", split='test') # maybe test -> test daten \n",
    "\n",
    "model = __models__[\"cfnet\"](284)\n",
    "model = nn.DataParallel(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "best_val_loss = -1\n",
    "\n",
    "# load parameters\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m      5\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[0;32m----> 6\u001b[0m preds, perturbed_inputs \u001b[39m=\u001b[39m perform_fgsm_attack(model, transform(image_left), transform(image_right), attack_epsilon)\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mperform_fgsm_attack\u001b[0;34m(model, left, right, attack_epsilon, targeted_disparity, targeted)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m         labels \u001b[39m=\u001b[39m model(left, right)[\u001b[39m\"\u001b[39m\u001b[39mdisparities\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39m# Instantiate the FGSM attack from torchattacks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m attack \u001b[39m=\u001b[39m FGSM(model, eps\u001b[39m=\u001b[39mattack_epsilon)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:511\u001b[0m, in \u001b[0;36mcfnet.forward\u001b[0;34m(self, left, right)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, left, right):\n\u001b[0;32m--> 511\u001b[0m     features_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extraction(left)\n\u001b[1;32m    512\u001b[0m     features_right \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extraction(right)\n\u001b[1;32m    514\u001b[0m     gwc_volume4 \u001b[39m=\u001b[39m build_gwc_volume(features_left[\u001b[39m\"\u001b[39m\u001b[39mgw4\u001b[39m\u001b[39m\"\u001b[39m], features_right[\u001b[39m\"\u001b[39m\u001b[39mgw4\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxdisp \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m8\u001b[39m,\n\u001b[1;32m    515\u001b[0m                                    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_groups)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:136\u001b[0m, in \u001b[0;36mfeature_extraction.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 136\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirstconv(x)\n\u001b[1;32m    137\u001b[0m     \u001b[39m# x = self.layer1(x)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     l2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)     \u001b[39m#1/2\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:142\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_input_dim(\u001b[39minput\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:425\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    424\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "image_left  = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "image_right = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "attack_epsilon = 0.03  # Epsilon value for the FGSM attack\n",
    "from torchvision import transforms\n",
    "transform = transforms.ToTensor()\n",
    "preds, perturbed_inputs = perform_fgsm_attack(model, transform(image_left), transform(image_right), attack_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 135 but got size 136 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m tensor_image_right\u001b[39m=\u001b[39m transform(image_right)\n\u001b[1;32m      9\u001b[0m tensor_image_righ\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(tensor_image_right, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m preds, perturbed_inputs \u001b[39m=\u001b[39m perform_fgsm_attack(model, tensor_image_left ,tensor_image_right , attack_epsilon)\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mperform_fgsm_attack\u001b[0;34m(model, left, right, attack_epsilon, targeted_disparity, targeted)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m         labels \u001b[39m=\u001b[39m model(left, right)[\u001b[39m\"\u001b[39m\u001b[39mdisparities\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39m# Instantiate the FGSM attack from torchattacks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m attack \u001b[39m=\u001b[39m FGSM(model, eps\u001b[39m=\u001b[39mattack_epsilon)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:511\u001b[0m, in \u001b[0;36mcfnet.forward\u001b[0;34m(self, left, right)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, left, right):\n\u001b[0;32m--> 511\u001b[0m     features_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extraction(left)\n\u001b[1;32m    512\u001b[0m     features_right \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extraction(right)\n\u001b[1;32m    514\u001b[0m     gwc_volume4 \u001b[39m=\u001b[39m build_gwc_volume(features_left[\u001b[39m\"\u001b[39m\u001b[39mgw4\u001b[39m\u001b[39m\"\u001b[39m], features_right[\u001b[39m\"\u001b[39m\u001b[39mgw4\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxdisp \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m8\u001b[39m,\n\u001b[1;32m    515\u001b[0m                                    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_groups)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:159\u001b[0m, in \u001b[0;36mfeature_extraction.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m decov_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miconv4(concat4)\n\u001b[1;32m    154\u001b[0m \u001b[39m# print(self.upconv4(decov_4).shape, l3.shape)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m# # torch.Size([1, 128, 112, 256]) torch.Size([1, 128, 109, 256])\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m# if self.upconv4(decov_4).shape[2] != l3.shape[2]:\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m#     l3 = F.pad(l3, (0, 0, 0, 3))\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m# print(self.upconv4(decov_4).shape, l3.shape)\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m concat3 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((l3, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupconv4(decov_4)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    160\u001b[0m decov_3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miconv3(concat3)\n\u001b[1;32m    161\u001b[0m \u001b[39m# print(self.upconv3(decov_3).shape, l2.shape)\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# # torch.Size([1, 64, 224, 512]) torch.Size([1, 64, 218, 512])\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# if self.upconv3(decov_3).shape[2] != l2.shape[2]:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m#     l2 = F.pad(l2, (0, 0, 0, 6))\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 135 but got size 136 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "image_left  = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "image_right = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "attack_epsilon = 0.03  # Epsilon value for the FGSM attack\n",
    "from torchvision import transforms\n",
    "transform = transforms.ToTensor()\n",
    "tensor_image_left= transform(image_left)\n",
    "tensor_image_left= torch.unsqueeze(tensor_image_left, 0)\n",
    "tensor_image_right= transform(image_right)\n",
    "tensor_image_righ= torch.unsqueeze(tensor_image_right, 0)\n",
    "preds, perturbed_inputs = perform_fgsm_attack(model, tensor_image_left ,tensor_image_right , attack_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m attack_epsilon \u001b[39m=\u001b[39m \u001b[39m0.03\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[39m# Perform the FGSM attack\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m preds, perturbed_inputs \u001b[39m=\u001b[39m perform_fgsm_attack(model, image_left_tensor, image_right_tensor, attack_epsilon)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mperform_fgsm_attack\u001b[0;34m(model, left, right, attack_epsilon, targeted_disparity, targeted)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m         labels \u001b[39m=\u001b[39m model(left, right)[\u001b[39m\"\u001b[39m\u001b[39mdisparities\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39m# Instantiate the FGSM attack from torchattacks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m attack \u001b[39m=\u001b[39m FGSM(model, eps\u001b[39m=\u001b[39mattack_epsilon)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:543\u001b[0m, in \u001b[0;36mcfnet.forward\u001b[0;34m(self, left, right)\u001b[0m\n\u001b[1;32m    541\u001b[0m cost0_6 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres0_6(volume6)\n\u001b[1;32m    542\u001b[0m cost0_6 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres1_6(cost0_6) \u001b[39m+\u001b[39m cost0_6\n\u001b[0;32m--> 543\u001b[0m out1_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine1(cost0_4, cost0_5, cost0_6)\n\u001b[1;32m    544\u001b[0m out2_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres3(out1_4)\n\u001b[1;32m    547\u001b[0m cost2_s4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassif2(out2_4)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:228\u001b[0m, in \u001b[0;36mhourglassup.forward\u001b[0;34m(self, x, feature4, feature5)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, feature4, feature5):\n\u001b[1;32m    227\u001b[0m     conv1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)          \u001b[39m#1/8\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     conv1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((conv1, feature4), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)   \u001b[39m#1/8\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     conv1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine1(conv1)   \u001b[39m#1/8\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     conv2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(conv1)      \u001b[39m#1/8\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchattacks import FGSM\n",
    "from torchvision import transforms\n",
    "from typing import Optional\n",
    "\n",
    "def perform_fgsm_attack(\n",
    "    model,\n",
    "    left: torch.Tensor,\n",
    "    right: torch.Tensor,\n",
    "    attack_epsilon: float,\n",
    "    targeted_disparity: Optional[torch.Tensor] = None,\n",
    "    targeted: bool = False,\n",
    "):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # If targeted, set the target, else use the model's own prediction as the target\n",
    "    if targeted:\n",
    "        labels = targeted_disparity.unsqueeze(0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            labels = model(left, right)[\"disparities\"]\n",
    "\n",
    "    # Instantiate the FGSM attack from torchattacks\n",
    "    attack = FGSM(model, eps=attack_epsilon)\n",
    "    \n",
    "    # If targeted, set the target label for the attack\n",
    "    if targeted:\n",
    "        attack.set_targeted_mode()\n",
    "\n",
    "    # Perform the attack\n",
    "    adv_left = attack(left, labels)\n",
    "    adv_right = attack(right, labels)\n",
    "\n",
    "    # Prepare the perturbed inputs for the model\n",
    "    perturbed_inputs = {\"left\": adv_left, \"right\": adv_right}\n",
    "\n",
    "    # Get predictions on the perturbed images\n",
    "    preds = model(perturbed_inputs[\"left\"], perturbed_inputs[\"right\"])\n",
    "\n",
    "    return preds, perturbed_inputs\n",
    "\n",
    "# Load and transform images\n",
    "image_left = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "image_right = train_dataset.load_image(train_dataset.img_right_filenames[0])\n",
    "common_size = (256, 256)  # Define the common size for resizing and cropping\n",
    "\n",
    "# Define the transformation including resize and center crop\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(common_size),\n",
    "    transforms.CenterCrop(common_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Transform the images and add a batch dimension\n",
    "image_left_tensor = transform(image_left).unsqueeze(0)  # Add batch dimension\n",
    "image_right_tensor = transform(image_right).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Epsilon value for the FGSM attack\n",
    "attack_epsilon = 0.03\n",
    "\n",
    "# Perform the FGSM attack\n",
    "preds, perturbed_inputs = perform_fgsm_attack(model, image_left_tensor, image_right_tensor, attack_epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m attack_epsilon \u001b[39m=\u001b[39m \u001b[39m0.03\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39m# Perform the FGSM attack\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m preds, perturbed_inputs \u001b[39m=\u001b[39m perform_fgsm_attack(model, image_left_tensor, image_right_tensor, attack_epsilon)\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mperform_fgsm_attack\u001b[0;34m(model, left, right, attack_epsilon, targeted_disparity, targeted)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m         labels \u001b[39m=\u001b[39m model(left, right)[\u001b[39m\"\u001b[39m\u001b[39mdisparities\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39m# Instantiate the FGSM attack from torchattacks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m attack \u001b[39m=\u001b[39m FGSM(model, eps\u001b[39m=\u001b[39mattack_epsilon)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:543\u001b[0m, in \u001b[0;36mcfnet.forward\u001b[0;34m(self, left, right)\u001b[0m\n\u001b[1;32m    541\u001b[0m cost0_6 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres0_6(volume6)\n\u001b[1;32m    542\u001b[0m cost0_6 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres1_6(cost0_6) \u001b[39m+\u001b[39m cost0_6\n\u001b[0;32m--> 543\u001b[0m out1_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine1(cost0_4, cost0_5, cost0_6)\n\u001b[1;32m    544\u001b[0m out2_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres3(out1_4)\n\u001b[1;32m    547\u001b[0m cost2_s4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassif2(out2_4)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:228\u001b[0m, in \u001b[0;36mhourglassup.forward\u001b[0;34m(self, x, feature4, feature5)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, feature4, feature5):\n\u001b[1;32m    227\u001b[0m     conv1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)          \u001b[39m#1/8\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     conv1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((conv1, feature4), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)   \u001b[39m#1/8\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     conv1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine1(conv1)   \u001b[39m#1/8\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     conv2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(conv1)      \u001b[39m#1/8\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchattacks import FGSM\n",
    "from torchvision import transforms\n",
    "from typing import Optional\n",
    "\n",
    "def perform_fgsm_attack(\n",
    "    model,\n",
    "    left: torch.Tensor,\n",
    "    right: torch.Tensor,\n",
    "    attack_epsilon: float,\n",
    "    targeted_disparity: Optional[torch.Tensor] = None,\n",
    "    targeted: bool = False,\n",
    "):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # If targeted, set the target, else use the model's own prediction as the target\n",
    "    if targeted:\n",
    "        labels = targeted_disparity.unsqueeze(0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            labels = model(left, right)[\"disparities\"]\n",
    "\n",
    "    # Instantiate the FGSM attack from torchattacks\n",
    "    attack = FGSM(model, eps=attack_epsilon)\n",
    "    \n",
    "    # If targeted, set the target label for the attack\n",
    "    if targeted:\n",
    "        attack.set_targeted_mode()\n",
    "\n",
    "    # Perform the attack\n",
    "    adv_left = attack(left, labels)\n",
    "    adv_right = attack(right, labels)\n",
    "\n",
    "    # Prepare the perturbed inputs for the model\n",
    "    perturbed_inputs = {\"left\": adv_left, \"right\": adv_right}\n",
    "\n",
    "    # Get predictions on the perturbed images\n",
    "    preds = model(perturbed_inputs[\"left\"], perturbed_inputs[\"right\"])\n",
    "\n",
    "    return preds, perturbed_inputs\n",
    "\n",
    "# Load and transform images\n",
    "image_left = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "image_right = train_dataset.load_image(train_dataset.img_right_filenames[0])\n",
    "\n",
    "# Ensure the images have the same size before transforming them to tensors\n",
    "resize_transform = transforms.Resize((256, 512))  # Adjust the size as needed to match model's expected input size\n",
    "image_left = resize_transform(image_left)\n",
    "image_right = resize_transform(image_right)\n",
    "\n",
    "# Define the transformation including to tensor conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Transform the images and add a batch dimension\n",
    "image_left_tensor = transform(image_left).unsqueeze(0)  # Add batch dimension\n",
    "image_right_tensor = transform(image_right).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Ensure the dimensions are consistent\n",
    "assert image_left_tensor.shape == image_right_tensor.shape, \\\n",
    "    f\"Shapes are not matching: {image_left_tensor.shape} vs {image_right_tensor.shape}\"\n",
    "\n",
    "# Epsilon value for the FGSM attack\n",
    "attack_epsilon = 0.03\n",
    "\n",
    "# Perform the FGSM attack\n",
    "preds, perturbed_inputs = perform_fgsm_attack(model, image_left_tensor, image_right_tensor, attack_epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list.\n",
      "Left image tensor shape: torch.Size([1, 3, 256, 512])\n",
      "Right image tensor shape: torch.Size([1, 3, 256, 512])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'feature_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     preds, perturbed_inputs \u001b[39m=\u001b[39m perform_fgsm_attack(model, image_left_tensor, image_right_tensor, attack_epsilon)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mperform_fgsm_attack\u001b[0;34m(model, left, right, attack_epsilon, targeted_disparity, targeted)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m         labels \u001b[39m=\u001b[39m model(left, right)[\u001b[39m\"\u001b[39m\u001b[39mdisparities\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39m# Instantiate the FGSM attack from torchattacks\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:543\u001b[0m, in \u001b[0;36mcfnet.forward\u001b[0;34m(self, left, right)\u001b[0m\n\u001b[1;32m    542\u001b[0m cost0_6 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres1_6(cost0_6) \u001b[39m+\u001b[39m cost0_6\n\u001b[0;32m--> 543\u001b[0m out1_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine1(cost0_4, cost0_5, cost0_6)\n\u001b[1;32m    544\u001b[0m out2_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdres3(out1_4)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/benchmarking_robustness/disparity_estimation/attacks/models/cfnet.py:228\u001b[0m, in \u001b[0;36mhourglassup.forward\u001b[0;34m(self, x, feature4, feature5)\u001b[0m\n\u001b[1;32m    227\u001b[0m conv1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)          \u001b[39m#1/8\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m conv1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((conv1, feature4), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)   \u001b[39m#1/8\u001b[39;00m\n\u001b[1;32m    229\u001b[0m conv1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine1(conv1)   \u001b[39m#1/8\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m# Check intermediate shapes if possible\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 78\u001b[0m     features_left \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfeature_extraction(image_left_tensor)\n\u001b[1;32m     79\u001b[0m     features_right \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfeature_extraction(image_right_tensor)\n\u001b[1;32m     80\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeatures left shape: \u001b[39m\u001b[39m{\u001b[39;00mfeatures_left[\u001b[39m'\u001b[39m\u001b[39mgw4\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/pfs/work7/workspace/scratch/ma_adackerm-team_project_fss2024/miniconda3/envs/unified_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'feature_extraction'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchattacks import FGSM\n",
    "from torchvision import transforms\n",
    "from typing import Optional\n",
    "\n",
    "def perform_fgsm_attack(\n",
    "    model,\n",
    "    left: torch.Tensor,\n",
    "    right: torch.Tensor,\n",
    "    attack_epsilon: float,\n",
    "    targeted_disparity: Optional[torch.Tensor] = None,\n",
    "    targeted: bool = False,\n",
    "):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # If targeted, set the target, else use the model's own prediction as the target\n",
    "    if targeted:\n",
    "        labels = targeted_disparity.unsqueeze(0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            labels = model(left, right)[\"disparities\"]\n",
    "\n",
    "    # Instantiate the FGSM attack from torchattacks\n",
    "    attack = FGSM(model, eps=attack_epsilon)\n",
    "    \n",
    "    # If targeted, set the target label for the attack\n",
    "    if targeted:\n",
    "        attack.set_targeted_mode()\n",
    "\n",
    "    # Perform the attack\n",
    "    adv_left = attack(left, labels)\n",
    "    adv_right = attack(right, labels)\n",
    "\n",
    "    # Prepare the perturbed inputs for the model\n",
    "    perturbed_inputs = {\"left\": adv_left, \"right\": adv_right}\n",
    "\n",
    "    # Get predictions on the perturbed images\n",
    "    preds = model(perturbed_inputs[\"left\"], perturbed_inputs[\"right\"])\n",
    "\n",
    "    return preds, perturbed_inputs\n",
    "\n",
    "# Load and transform images\n",
    "image_left = train_dataset.load_image(train_dataset.img_left_filenames[0])\n",
    "image_right = train_dataset.load_image(train_dataset.img_right_filenames[0])\n",
    "\n",
    "# Ensure the images have the same size before transforming them to tensors\n",
    "resize_transform = transforms.Resize((256, 512))  # Adjust the size as needed to match model's expected input size\n",
    "image_left = resize_transform(image_left)\n",
    "image_right = resize_transform(image_right)\n",
    "\n",
    "# Define the transformation including to tensor conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Transform the images and add a batch dimension\n",
    "image_left_tensor = transform(image_left).unsqueeze(0)  # Add batch dimension\n",
    "image_right_tensor = transform(image_right).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Ensure the dimensions are consistent\n",
    "assert image_left_tensor.shape == image_right_tensor.shape, \\\n",
    "    f\"Shapes are not matching: {image_left_tensor.shape} vs {image_right_tensor.shape}\"\n",
    "\n",
    "# Epsilon value for the FGSM attack\n",
    "attack_epsilon = 0.03\n",
    "\n",
    "# Perform the FGSM attack\n",
    "try:\n",
    "    preds, perturbed_inputs = perform_fgsm_attack(model, image_left_tensor, image_right_tensor, attack_epsilon)\n",
    "except RuntimeError as e:\n",
    "    print(f\"RuntimeError: {e}\")\n",
    "    print(f\"Left image tensor shape: {image_left_tensor.shape}\")\n",
    "    print(f\"Right image tensor shape: {image_right_tensor.shape}\")\n",
    "\n",
    "    # Check intermediate shapes if possible\n",
    "    with torch.no_grad():\n",
    "        features_left = model.feature_extraction(image_left_tensor)\n",
    "        features_right = model.feature_extraction(image_right_tensor)\n",
    "        print(f\"Features left shape: {features_left['gw4'].shape}\")\n",
    "        print(f\"Features right shape: {features_right['gw4'].shape}\")\n",
    "\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unified_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
